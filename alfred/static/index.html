<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Alfred Global Assistant</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      background: #0f172a;
      color: #e5e7eb;
      margin: 0;
      display: flex;
      flex-direction: column;
      height: 100vh;
    }
    header {
      padding: 12px 16px;
      background: #020617;
      border-bottom: 1px solid #1f2937;
      font-weight: 600;
      font-size: 18px;
    }
    #chat {
      flex: 1;
      overflow-y: auto;
      padding: 16px;
    }
    .bubble {
      max-width: 80%;
      padding: 10px 12px;
      border-radius: 12px;
      margin-bottom: 8px;
      line-height: 1.4;
      font-size: 14px;
      white-space: pre-wrap;
    }
    .user {
      background: #22c55e;
      color: #022c22;
      margin-left: auto;
      border-bottom-right-radius: 2px;
    }
    .alfred {
      background: #111827;
      border: 1px solid #1f2937;
      margin-right: auto;
      border-bottom-left-radius: 2px;
    }
    form {
      display: flex;
      padding: 10px;
      border-top: 1px solid #1f2937;
      background: #020617;
      gap: 8px;
    }
    input[type="text"] {
      flex: 1;
      padding: 8px 10px;
      border-radius: 999px;
      border: 1px solid #374151;
      background: #020617;
      color: #e5e7eb;
      outline: none;
    }
    select {
      padding: 8px 10px;
      border-radius: 999px;
      border: 1px solid #374151;
      background: #020617;
      color: #e5e7eb;
      outline: none;
      max-width: 220px;
    }
    #mic-meter {
      height: 10px;
      width: 90px;
      border-radius: 999px;
      border: 1px solid #374151;
      background: #020617;
      overflow: hidden;
      align-self: center;
    }
    #mic-meter > div {
      height: 100%;
      width: 0%;
      background: #22c55e;
      transition: width 0.08s linear;
    }
    button {
      padding: 8px 16px;
      border-radius: 999px;
      border: none;
      background: #22c55e;
      color: #022c22;
      font-weight: 600;
      cursor: pointer;
    }
    button:disabled {
      opacity: 0.6;
      cursor: default;
    }
  </style>
</head>
<body>
  <header>Alfred Global Assistant</header>
  <div id="chat"></div>

  <form id="chat-form">
    <button type="button" id="mic-btn" title="Hold to talk">üé§</button>
    <button type="button" id="handsfree-btn" title="Toggle hands-free listening">Hands‚Äëfree: Off</button>
    <select id="mic-device" title="Select microphone">
      <option value="">Mic: Default</option>
    </select>
    <div id="mic-meter" title="Mic level (hands-free)"><div></div></div>
    <input id="message" type="text" placeholder="Talk to Alfred..." autocomplete="off" />
    <button type="button" id="stop-tts-btn" title="Stop speaking" disabled>Stop</button>
    <button type="button" id="pause-tts-btn" title="Pause speaking" disabled>Pause</button>
    <button type="button" id="resume-tts-btn" title="Continue speaking" disabled>Continue</button>
    <button type="submit">Send</button>
  </form>

  <script>
  const API_BASE = (window.location.origin && window.location.origin !== "null")
    ? window.location.origin
    : "http://127.0.0.1:8000";
  const chat = document.getElementById("chat");
  const form = document.getElementById("chat-form");
  const input = document.getElementById("message");
  const button = form.querySelector('button[type="submit"]');
  const micBtn = document.getElementById("mic-btn");
  const handsFreeBtn = document.getElementById("handsfree-btn");
  const micDeviceSelect = document.getElementById("mic-device");
  const micMeter = document.getElementById("mic-meter");
  const micMeterFill = micMeter ? micMeter.querySelector("div") : null;
  const stopTtsBtn = document.getElementById("stop-tts-btn");
  const pauseTtsBtn = document.getElementById("pause-tts-btn");
  const resumeTtsBtn = document.getElementById("resume-tts-btn");

  let currentAudio = null;
  let currentAudioUrl = null;

  function setTtsControls({ active, paused }) {
    stopTtsBtn.disabled = !active;
    pauseTtsBtn.disabled = !active || paused;
    resumeTtsBtn.disabled = !active || !paused;
  }

  function stopTts() {
    try {
      if ("speechSynthesis" in window) {
        window.speechSynthesis.cancel();
      }
      if (currentAudio) {
        currentAudio.pause();
        currentAudio.currentTime = 0;
        currentAudio = null;
      }
      if (currentAudioUrl) {
        URL.revokeObjectURL(currentAudioUrl);
        currentAudioUrl = null;
      }
    } finally {
      setTtsControls({ active: false, paused: false });
    }
  }

  function pauseTts() {
    let paused = false;
    try {
      if ("speechSynthesis" in window && window.speechSynthesis.speaking && !window.speechSynthesis.paused) {
        window.speechSynthesis.pause();
        paused = true;
      } else if (currentAudio && !currentAudio.paused) {
        currentAudio.pause();
        paused = true;
      }
    } finally {
      if (stopTtsBtn.disabled === false) {
        setTtsControls({ active: true, paused });
      }
    }
  }

  async function resumeTts() {
    let paused = false;
    try {
      if ("speechSynthesis" in window && window.speechSynthesis.paused) {
        window.speechSynthesis.resume();
        paused = false;
      } else if (currentAudio && currentAudio.paused) {
        await currentAudio.play();
        paused = false;
      }
    } catch (e) {
      // fall through
    } finally {
      if (stopTtsBtn.disabled === false) {
        setTtsControls({ active: true, paused });
      }
    }
  }

  function handleVoiceControlCommand(transcriptRaw) {
    const t = (transcriptRaw || "").trim().toLowerCase();
    if (!t) return false;

    const stopPhrases = new Set(["stop", "stop speaking", "stop tts"]);
    const pausePhrases = new Set(["pause", "pause speaking", "pause tts"]);
    const resumePhrases = new Set(["continue", "resume", "continue speaking", "resume speaking", "continue tts", "resume tts"]);

    if (stopPhrases.has(t)) {
      stopTts();
      return true;
    }
    if (pausePhrases.has(t)) {
      pauseTts();
      return true;
    }
    if (resumePhrases.has(t)) {
      // resume can be async; fire-and-forget for UI
      resumeTts();
      return true;
    }
    return false;
  }

  let recorder = null;
  let recording = false;
  let chunks = [];

  // Mic device selection (helps fix "permission works but audio is silent" on Windows)
  const MIC_DEVICE_STORAGE_KEY = "alfred_mic_device_id";
  let selectedMicDeviceId = "";

  function buildAudioConstraints() {
    const audio = {
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true,
    };
    if (selectedMicDeviceId) {
      audio.deviceId = { exact: selectedMicDeviceId };
    }
    return audio;
  }

  async function refreshMicDeviceList() {
    if (!navigator.mediaDevices?.enumerateDevices) return;
    let devices = [];
    try {
      devices = await navigator.mediaDevices.enumerateDevices();
    } catch (e) {
      return;
    }
    const inputs = devices.filter((d) => d && d.kind === "audioinput");

    const currentValue = micDeviceSelect.value;
    micDeviceSelect.innerHTML = "";

    const optDefault = document.createElement("option");
    optDefault.value = "";
    optDefault.textContent = "Mic: Default";
    micDeviceSelect.appendChild(optDefault);

    for (const d of inputs) {
      const opt = document.createElement("option");
      opt.value = d.deviceId || "";
      opt.textContent = d.label ? `Mic: ${d.label}` : "Mic: (permission needed to show names)";
      micDeviceSelect.appendChild(opt);
    }

    // Try to keep selection stable.
    const preferred = selectedMicDeviceId || currentValue;
    if (preferred && Array.from(micDeviceSelect.options).some((o) => o.value === preferred)) {
      micDeviceSelect.value = preferred;
    } else {
      micDeviceSelect.value = "";
      selectedMicDeviceId = "";
      try { localStorage.removeItem(MIC_DEVICE_STORAGE_KEY); } catch (e) {}
    }
  }

  // Hands-free mode (wake word: "yow alfred")
  const WAKE_PHRASE = "yow alfred";
  let handsFreeEnabled = false;
  let handsFreeStream = null;
  let handsFreeLoopBusy = false;
  let handsFreeNextIsCommand = false;
  let handsFreeBackoffMs = 500;

  // Hands-free diagnostics / silence detection
  let handsFreeAudioCtx = null;
  let handsFreeAnalyser = null;
  let handsFreeSource = null;
  let handsFreeSilentStreak = 0;
  let lastHandsFreeDiagAt = 0;
  let handsFreeMeterRaf = 0;

  function addBubble(text, role) {
    const div = document.createElement("div");
    div.className = `bubble ${role}`;
    div.textContent = text;
    chat.appendChild(div);
    chat.scrollTop = chat.scrollHeight;
  }

  function updateHandsFreeUi() {
    handsFreeBtn.textContent = handsFreeEnabled ? "Hands‚Äëfree: On" : "Hands‚Äëfree: Off";
    handsFreeBtn.title = handsFreeEnabled
      ? `Hands‚Äëfree listening is ON (wake phrase: "${WAKE_PHRASE}")`
      : "Toggle hands-free listening";
    micBtn.disabled = handsFreeEnabled;
    if (micMeter) micMeter.style.opacity = handsFreeEnabled ? "1" : "0.55";
  }

  function ensureMicSecureContext() {
    // Localhost is generally OK, but non-localhost HTTP can block mic.
    if (window.isSecureContext) return true;
    const origin = (window.location && window.location.origin) ? window.location.origin : "(unknown origin)";
    addBubble(`‚ö†Ô∏è Microphone blocked: not a secure context. Use HTTPS or localhost. Current origin: ${origin}`, "alfred");
    return false;
  }

  function escapeRegExp(s) {
    return String(s).replace(/[.*+?^${}()|[\]\\]/g, "\\$&");
  }

  function pickMediaRecorderMimeType() {
    if (!window.MediaRecorder || typeof MediaRecorder.isTypeSupported !== "function") return "";
    const candidates = [
      "audio/webm;codecs=opus",
      "audio/webm",
      "audio/ogg;codecs=opus",
      "audio/ogg",
      "audio/mp4",
    ];
    for (const t of candidates) {
      try {
        if (MediaRecorder.isTypeSupported(t)) return t;
      } catch (e) {}
    }
    return "";
  }

  function getRmsLevel(analyser) {
    const data = new Uint8Array(analyser.fftSize);
    analyser.getByteTimeDomainData(data);
    let sumSq = 0;
    for (let i = 0; i < data.length; i++) {
      const v = (data[i] - 128) / 128;
      sumSq += v * v;
    }
    return Math.sqrt(sumSq / data.length);
  }

  async function sampleMaxRms(durationMs) {
    if (!handsFreeAnalyser) return null;
    const steps = Math.max(1, Math.floor(durationMs / 40));
    let max = 0;
    for (let i = 0; i < steps; i++) {
      max = Math.max(max, getRmsLevel(handsFreeAnalyser));
      await new Promise((r) => setTimeout(r, 40));
    }
    return max;
  }

  function getSelectedMicLabel() {
    try {
      const opt = micDeviceSelect && micDeviceSelect.options ? micDeviceSelect.options[micDeviceSelect.selectedIndex] : null;
      return opt && opt.textContent ? opt.textContent : "Mic: Default";
    } catch (e) {
      return "Mic: Default";
    }
  }

  function startHandsFreeMeter() {
    if (!micMeterFill) return;
    if (handsFreeMeterRaf) cancelAnimationFrame(handsFreeMeterRaf);
    const tick = () => {
      if (!handsFreeEnabled || !handsFreeAnalyser) {
        micMeterFill.style.width = "0%";
        handsFreeMeterRaf = 0;
        return;
      }
      const rms = getRmsLevel(handsFreeAnalyser);
      // Map rms to a visible percentage; clamp.
      const pct = Math.max(0, Math.min(100, Math.round((rms / 0.08) * 100)));
      micMeterFill.style.width = `${pct}%`;
      handsFreeMeterRaf = requestAnimationFrame(tick);
    };
    handsFreeMeterRaf = requestAnimationFrame(tick);
  }

  function stopHandsFreeMeter() {
    if (handsFreeMeterRaf) cancelAnimationFrame(handsFreeMeterRaf);
    handsFreeMeterRaf = 0;
    if (micMeterFill) micMeterFill.style.width = "0%";
  }

  function encodeWavFromFloat32(samplesFloat32, sampleRate) {
    // 16-bit PCM mono WAV
    const numSamples = samplesFloat32.length;
    const bytesPerSample = 2;
    const blockAlign = bytesPerSample;
    const byteRate = sampleRate * blockAlign;
    const dataSize = numSamples * bytesPerSample;
    const buffer = new ArrayBuffer(44 + dataSize);
    const view = new DataView(buffer);

    function writeAscii(offset, str) {
      for (let i = 0; i < str.length; i++) view.setUint8(offset + i, str.charCodeAt(i));
    }

    writeAscii(0, "RIFF");
    view.setUint32(4, 36 + dataSize, true);
    writeAscii(8, "WAVE");
    writeAscii(12, "fmt ");
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, 1, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, byteRate, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, 16, true);
    writeAscii(36, "data");
    view.setUint32(40, dataSize, true);

    let offset = 44;
    for (let i = 0; i < numSamples; i++) {
      let s = samplesFloat32[i];
      if (s > 1) s = 1;
      if (s < -1) s = -1;
      const int16 = s < 0 ? s * 0x8000 : s * 0x7fff;
      view.setInt16(offset, int16, true);
      offset += 2;
    }
    return buffer;
  }

  async function playAlfredVoice(text) {
    // Always stop any existing playback first
    stopTts();
    setTtsControls({ active: true, paused: false });

    // Prefer browser TTS when available to avoid server API usage during development
    if ("speechSynthesis" in window) {
      try {
        const utter = new SpeechSynthesisUtterance(text);
        // Optional: tune voice, rate, pitch
        utter.lang = "en-US";
        utter.rate = 1.0;
        utter.onend = () => setTtsControls({ active: false, paused: false });
        utter.onerror = () => setTtsControls({ active: false, paused: false });
        window.speechSynthesis.speak(utter);
        return;
      } catch (err) {
        console.warn("Browser TTS failed, falling back to server TTS:", err);
      }
    }

    // Fallback: server-side TTS (requires OPENAI_API_KEY and server support)
    try {
      const res = await fetch(`${API_BASE}/tts`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text, voice: "alloy", format: "mp3" }),
      });

      if (!res.ok) {
        console.error("TTS error:", await res.text());
        return;
      }

      const blob = await res.blob();
      currentAudioUrl = URL.createObjectURL(blob);
      currentAudio = new Audio(currentAudioUrl);
      currentAudio.onended = () => {
        setTtsControls({ active: false, paused: false });
        try {
          if (currentAudioUrl) URL.revokeObjectURL(currentAudioUrl);
        } catch (e) {}
        currentAudioUrl = null;
        currentAudio = null;
      };
      await currentAudio.play();
    } catch (err) {
      console.error("Error playing TTS:", err);
      setTtsControls({ active: false, paused: false });
    }
  }

  stopTtsBtn.addEventListener("click", () => {
    stopTts();
  });

  pauseTtsBtn.addEventListener("click", () => {
    pauseTts();
  });

  resumeTtsBtn.addEventListener("click", () => {
    resumeTts();
  });

  form.addEventListener("submit", async (e) => {
    e.preventDefault();
    const text = input.value.trim();
    if (!text) return;

    addBubble(text, "user");
    input.value = "";
    input.focus();
    button.disabled = true;

    try {
      const res = await fetch(`${API_BASE}/chat`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ user_id: "default", message: text }),
      });
      const data = await res.json();
      addBubble(data.reply, "alfred");
      // Trigger Alfred's voice
      playAlfredVoice(data.reply);
    } catch (err) {
      addBubble("‚ö†Ô∏è Error talking to Alfred API.", "alfred");
      console.error(err);
    } finally {
      button.disabled = false;
    }
  });

  // ---- Voice input using API /stt via MediaRecorder ----
  function setMicUiActive(active) {
    recording = active;
    if (active) {
      micBtn.classList.add("listening");
      micBtn.textContent = "‚óè";
      micBtn.title = "Click to stop recording";
    } else {
      micBtn.classList.remove("listening");
      micBtn.textContent = "üé§";
      micBtn.title = "Click to record";
    }
  }

  async function sttFromBlob(blob, filename) {
    const fd = new FormData();
    fd.append("audio", blob, filename);

    const res = await fetch(`${API_BASE}/stt`, {
      method: "POST",
      body: fd,
    });

    if (!res.ok) {
      const text = await res.text();
      const err = new Error(text);
      err.status = res.status;
      throw err;
    }
    return res.json();
  }

  async function recordOnceFromStream(stream, durationMs) {
    // Try MediaRecorder first; if it fails, fall back to WAV via WebAudio.
    if (window.MediaRecorder) {
      try {
        return await new Promise((resolve, reject) => {
          let localChunks = [];
          let localRecorder;
          const mimeType = pickMediaRecorderMimeType();
          try {
            localRecorder = mimeType ? new MediaRecorder(stream, { mimeType }) : new MediaRecorder(stream);
          } catch (e) {
            reject(e);
            return;
          }
          localRecorder.ondataavailable = (e) => {
            if (e.data && e.data.size > 0) localChunks.push(e.data);
          };
          localRecorder.onerror = (e) => reject(e);
          localRecorder.onstop = () => {
            resolve(new Blob(localChunks, { type: localRecorder.mimeType || "audio/webm" }));
          };
          try {
            // timeslice helps ensure ondataavailable fires reliably across browsers
            localRecorder.start(250);
          } catch (e) {
            reject(e);
            return;
          }
          setTimeout(() => {
            try { localRecorder.stop(); } catch (e) { reject(e); }
          }, durationMs);
        });
      } catch (e) {
        // fall through to WAV
      }
    }

    // WAV fallback
    return new Promise(async (resolve, reject) => {
      let audioCtx;
      try {
        audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      } catch (e) {
        reject(e);
        return;
      }
      let source;
      try {
        source = audioCtx.createMediaStreamSource(stream);
      } catch (e) {
        try { await audioCtx.close(); } catch (_) {}
        reject(e);
        return;
      }

      const processor = audioCtx.createScriptProcessor(4096, 1, 1);
      const samples = [];
      let total = 0;

      processor.onaudioprocess = (e) => {
        const input = e.inputBuffer.getChannelData(0);
        const copy = new Float32Array(input.length);
        copy.set(input);
        samples.push(copy);
        total += copy.length;
      };

      source.connect(processor);
      processor.connect(audioCtx.destination);
      try { await audioCtx.resume(); } catch (e) {}

      setTimeout(async () => {
        try { processor.disconnect(); source.disconnect(); } catch (e) {}
        const merged = new Float32Array(total);
        let offset = 0;
        for (const chunk of samples) {
          merged.set(chunk, offset);
          offset += chunk.length;
        }
        const wavBuffer = encodeWavFromFloat32(merged, audioCtx.sampleRate);
        try { await audioCtx.close(); } catch (e) {}
        resolve(new Blob([wavBuffer], { type: "audio/wav" }));
      }, durationMs);
    });
  }

  function extractWakeMessage(transcriptLower) {
    // Accept small variations like "yo, alfred" or "yow-alfred".
    const re = /\byo+w[\s,\.\-]+alfred\b/i;
    const m = transcriptLower.match(re);
    if (!m) return null;
    const idx = transcriptLower.indexOf(m[0].toLowerCase());
    const after = transcriptLower.slice(idx + m[0].length).trim();
    return { wake: m[0], after };
  }

  async function handsFreeLoop() {
    if (!handsFreeEnabled || !handsFreeStream || handsFreeLoopBusy) return;
    handsFreeLoopBusy = true;
    try {
      // Quick silence check to avoid spamming /stt with empty audio.
      const maxRms = await sampleMaxRms(350);
      if (maxRms !== null && maxRms < 0.003) {
        handsFreeSilentStreak += 1;
        // Only show a diagnostic occasionally.
        if (handsFreeSilentStreak >= 3) {
          const now = Date.now();
          if (now - lastHandsFreeDiagAt > 10000) {
            lastHandsFreeDiagAt = now;
            let trackInfo = "";
            try {
              const t = handsFreeStream && handsFreeStream.getAudioTracks ? handsFreeStream.getAudioTracks()[0] : null;
              const s = t && t.getSettings ? t.getSettings() : null;
              if (s) trackInfo = ` settings=${JSON.stringify(s)}`;
            } catch (e) {}
            addBubble(
              `‚ö†Ô∏è Hands‚Äëfree: mic input seems silent (maxRms=${maxRms.toFixed(4)}). ${getSelectedMicLabel()}${trackInfo}. If this stays near 0.0000, switch the mic dropdown or unmute the device in Windows Sound settings.`,
              "alfred"
            );
          }
        }
        return;
      }

      const blob = await recordOnceFromStream(handsFreeStream, 1400);
      if (!blob || blob.size < 2048) {
        handsFreeSilentStreak += 1;
        const now = Date.now();
        if (now - lastHandsFreeDiagAt > 10000) {
          lastHandsFreeDiagAt = now;
          addBubble("‚ö†Ô∏è Hands‚Äëfree: captured almost no audio. Try speaking closer to the mic.", "alfred");
        }
        return;
      }

      handsFreeSilentStreak = 0;

      const ext = (blob.type && blob.type.includes("ogg")) ? "ogg" : ((blob.type && blob.type.includes("mp4")) ? "mp4" : ((blob.type && blob.type.includes("wav")) ? "wav" : "webm"));
      const data = await sttFromBlob(blob, `handsfree.${ext}`);
      const transcript = (data && data.text ? String(data.text) : "").trim();
      const transcriptLower = transcript.toLowerCase();
      if (!transcript) {
        const now = Date.now();
        if (now - lastHandsFreeDiagAt > 10000) {
          lastHandsFreeDiagAt = now;
          addBubble(
            `‚ö†Ô∏è Hands‚Äëfree: STT returned empty text (audio type=${blob.type || "unknown"}, bytes=${blob.size}).`,
            "alfred"
          );
        }
        return;
      }

      handsFreeBackoffMs = 500;

      // Allow voice control of TTS anytime.
      if (handleVoiceControlCommand(transcript)) return;

      if (handsFreeNextIsCommand) {
        handsFreeNextIsCommand = false;
        input.value = transcript;
        form.dispatchEvent(new Event("submit"));
        return;
      }

      const wake = extractWakeMessage(transcriptLower);
      if (!wake) return;

      if (wake.after) {
        input.value = transcript.replace(new RegExp(escapeRegExp(wake.wake), "i"), "").trim();
        if (input.value) form.dispatchEvent(new Event("submit"));
        return;
      }

      handsFreeNextIsCommand = true;
    } catch (err) {
      console.error("Hands-free STT loop error:", err);
      const status = (err && typeof err === "object" && "status" in err) ? err.status : null;
      const msg = (err && err.message) ? String(err.message) : String(err);
      const shortMsg = msg.length > 220 ? msg.slice(0, 220) + "‚Ä¶" : msg;

      // Stop on clear auth/key failures; otherwise retry with backoff.
      if (status === 401 || status === 403 || (status === 500 && shortMsg.toLowerCase().includes("api key"))) {
        setHandsFree(false);
        addBubble(`‚ö†Ô∏è Hands‚Äëfree stopped (STT unavailable): ${shortMsg}`, "alfred");
        return;
      }

      handsFreeBackoffMs = status === 429 ? 6000 : 2500;
      addBubble(`‚ö†Ô∏è Hands‚Äëfree STT error (retrying)${status ? ` [${status}]` : ""}: ${shortMsg}`, "alfred");
    } finally {
      handsFreeLoopBusy = false;
      if (handsFreeEnabled) setTimeout(handsFreeLoop, handsFreeBackoffMs);
    }
  }

  async function setHandsFree(enabled) {
    if (enabled === handsFreeEnabled) return;

    if (enabled) {
      if (!navigator.mediaDevices?.getUserMedia) {
        addBubble("‚ö†Ô∏è Hands‚Äëfree not supported in this browser.", "alfred");
        return;
      }
      if (!ensureMicSecureContext()) return;

      try {
        // Stop manual recording first.
        if (recorder && recording) {
          try { recorder.stop(); } catch (e) {}
        }
        handsFreeStream = await navigator.mediaDevices.getUserMedia({
          audio: buildAudioConstraints(),
        });

        // After permission is granted, refresh device list so labels appear.
        refreshMicDeviceList();

        // Set up analyser so we can detect silence (and diagnose ‚Äúmic works but Alfred hears nothing‚Äù).
        try {
          handsFreeAudioCtx = new (window.AudioContext || window.webkitAudioContext)();
          handsFreeSource = handsFreeAudioCtx.createMediaStreamSource(handsFreeStream);
          handsFreeAnalyser = handsFreeAudioCtx.createAnalyser();
          handsFreeAnalyser.fftSize = 1024;
          handsFreeSource.connect(handsFreeAnalyser);
          await handsFreeAudioCtx.resume();
        } catch (e) {
          handsFreeAudioCtx = null;
          handsFreeSource = null;
          handsFreeAnalyser = null;
        }
        startHandsFreeMeter();
        handsFreeEnabled = true;
        handsFreeNextIsCommand = false;
        handsFreeSilentStreak = 0;
        updateHandsFreeUi();
        handsFreeLoop();
      } catch (e) {
        console.error("Hands-free mic permission error:", e);
        addBubble("‚ö†Ô∏è Microphone permission denied.", "alfred");
        handsFreeEnabled = false;
        handsFreeStream = null;
        updateHandsFreeUi();
      }
      return;
    }

    // disable
    handsFreeEnabled = false;
    handsFreeNextIsCommand = false;
    handsFreeSilentStreak = 0;
    try {
      if (handsFreeAudioCtx) {
        handsFreeAudioCtx.close();
      }
    } catch (e) {}
    handsFreeAudioCtx = null;
    handsFreeSource = null;
    handsFreeAnalyser = null;
    stopHandsFreeMeter();
    try {
      if (handsFreeStream) handsFreeStream.getTracks().forEach((t) => t.stop());
    } catch (e) {}
    handsFreeStream = null;
    updateHandsFreeUi();
  }

  async function startRecording() {
    if (!navigator.mediaDevices?.getUserMedia) {
      micBtn.disabled = true;
      micBtn.title = "Voice recording not supported in this browser";
      return;
    }
    if (!ensureMicSecureContext()) return;

    const stream = await navigator.mediaDevices.getUserMedia({
      audio: buildAudioConstraints(),
    });

    // After permission is granted, refresh device list so labels appear.
    refreshMicDeviceList();

    // Prefer MediaRecorder for manual mic; fall back to recordOnceFromStream.
    if (window.MediaRecorder) {
      chunks = [];
      const mimeType = pickMediaRecorderMimeType();
      recorder = mimeType ? new MediaRecorder(stream, { mimeType }) : new MediaRecorder(stream);

      recorder.ondataavailable = (e) => {
        if (e.data && e.data.size > 0) chunks.push(e.data);
      };

      recorder.onstop = async () => {
        try {
          stream.getTracks().forEach((t) => t.stop());
          const blob = new Blob(chunks, { type: recorder.mimeType || "audio/webm" });
          const ext = (blob.type && blob.type.includes("ogg")) ? "ogg" : ((blob.type && blob.type.includes("mp4")) ? "mp4" : "webm");
          const data = await sttFromBlob(blob, `mic.${ext}`);
          const transcript = (data && data.text ? String(data.text) : "").trim();
          if (!transcript) {
            addBubble("‚ö†Ô∏è No transcript returned.", "alfred");
            return;
          }

          if (handleVoiceControlCommand(transcript)) return;
          input.value = transcript;
          form.dispatchEvent(new Event("submit"));
        } catch (err) {
          console.error("STT error:", err);
          const status = (err && typeof err === "object" && "status" in err) ? err.status : null;
          const msg = (err && err.message) ? String(err.message) : String(err);
          const shortMsg = msg.length > 220 ? msg.slice(0, 220) + "‚Ä¶" : msg;
          addBubble(`‚ö†Ô∏è Error transcribing audio (STT)${status ? ` [${status}]` : ""}: ${shortMsg}`, "alfred");
        } finally {
          setMicUiActive(false);
          recorder = null;
        }
      };

      recorder.start(250);
      setMicUiActive(true);
      return;
    }

    // Fallback path
    try {
      setMicUiActive(true);
      const blob = await recordOnceFromStream(stream, 1800);
      stream.getTracks().forEach((t) => t.stop());
      const data = await sttFromBlob(blob, "mic.wav");
      const transcript = (data && data.text ? String(data.text) : "").trim();
      if (!transcript) {
        addBubble("‚ö†Ô∏è No transcript returned.", "alfred");
        return;
      }
      if (handleVoiceControlCommand(transcript)) return;
      input.value = transcript;
      form.dispatchEvent(new Event("submit"));
    } catch (err) {
      console.error("STT error:", err);
      const status = (err && typeof err === "object" && "status" in err) ? err.status : null;
      const msg = (err && err.message) ? String(err.message) : String(err);
      const shortMsg = msg.length > 220 ? msg.slice(0, 220) + "‚Ä¶" : msg;
      addBubble(`‚ö†Ô∏è Error transcribing audio (STT)${status ? ` [${status}]` : ""}: ${shortMsg}`, "alfred");
    } finally {
      setMicUiActive(false);
      recorder = null;
      try { stream.getTracks().forEach((t) => t.stop()); } catch (e) {}
    }
  }

  function stopRecording() {
    if (recorder && recording) {
      recorder.stop();
    }
  }

  micBtn.addEventListener("click", async () => {
    try {
      if (!recording) {
        await startRecording();
      } else {
        stopRecording();
      }
    } catch (err) {
      console.error("Mic error:", err);
      setMicUiActive(false);
      addBubble("‚ö†Ô∏è Microphone permission or recording failed.", "alfred");
    }
  });

  handsFreeBtn.addEventListener("click", async () => {
    await setHandsFree(!handsFreeEnabled);
  });

  micDeviceSelect.addEventListener("change", async () => {
    selectedMicDeviceId = micDeviceSelect.value || "";
    try {
      if (selectedMicDeviceId) localStorage.setItem(MIC_DEVICE_STORAGE_KEY, selectedMicDeviceId);
      else localStorage.removeItem(MIC_DEVICE_STORAGE_KEY);
    } catch (e) {}

    // If hands-free is running, restart it to apply the new device.
    if (handsFreeEnabled) {
      await setHandsFree(false);
      await setHandsFree(true);
    }
  });

  // Init
  setMicUiActive(false);
  setTtsControls({ active: false, paused: false });
  updateHandsFreeUi();
  try { selectedMicDeviceId = localStorage.getItem(MIC_DEVICE_STORAGE_KEY) || ""; } catch (e) { selectedMicDeviceId = ""; }
  refreshMicDeviceList();
  if (navigator.mediaDevices?.addEventListener) {
    navigator.mediaDevices.addEventListener("devicechange", refreshMicDeviceList);
  }
  addBubble("Hello, I'm Alfred. You can type or tap üé§ to talk to me.", "alfred");
  </script>
</body>
</html>
