<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Alfred Global Assistant</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      background: #0f172a;
      color: #e5e7eb;
      margin: 0;
      display: flex;
      flex-direction: column;
      height: 100vh;
    }
    header {
      padding: 12px 16px;
      background: #020617;
      border-bottom: 1px solid #1f2937;
      font-weight: 600;
      font-size: 18px;
    }
    #chat {
      flex: 1;
      overflow-y: auto;
      padding: 16px;
    }
    .bubble {
      max-width: 80%;
      padding: 10px 12px;
      border-radius: 12px;
      margin-bottom: 8px;
      line-height: 1.4;
      font-size: 14px;
      white-space: pre-wrap;
    }
    .user {
      background: #22c55e;
      color: #022c22;
      margin-left: auto;
      border-bottom-right-radius: 2px;
    }
    .alfred {
      background: #111827;
      border: 1px solid #1f2937;
      margin-right: auto;
      border-bottom-left-radius: 2px;
    }
    form {
      display: flex;
      padding: 10px;
      border-top: 1px solid #1f2937;
      background: #020617;
      gap: 8px;
    }
    input[type="text"] {
      flex: 1;
      padding: 8px 10px;
      border-radius: 999px;
      border: 1px solid #374151;
      background: #020617;
      color: #e5e7eb;
      outline: none;
    }
    button {
      padding: 8px 16px;
      border-radius: 999px;
      border: none;
      background: #22c55e;
      color: #022c22;
      font-weight: 600;
      cursor: pointer;
    }
    button:disabled {
      opacity: 0.6;
      cursor: default;
    }
  </style>
</head>
<body>
  <header>Alfred Global Assistant</header>
  <div id="chat"></div>

  <form id="chat-form">
    <button type="button" id="mic-btn" title="Hold to talk">ðŸŽ¤</button>
    <button type="button" id="handsfree-btn" title="Toggle hands-free listening">Handsâ€‘free: Off</button>
    <input id="message" type="text" placeholder="Talk to Alfred..." autocomplete="off" />
    <button type="button" id="stop-tts-btn" title="Stop speaking" disabled>Stop</button>
    <button type="button" id="pause-tts-btn" title="Pause speaking" disabled>Pause</button>
    <button type="button" id="resume-tts-btn" title="Continue speaking" disabled>Continue</button>
    <button type="submit">Send</button>
  </form>

  <script>
  const API_BASE = (window.location.origin && window.location.origin !== "null")
    ? window.location.origin
    : "http://127.0.0.1:8000";
  const chat = document.getElementById("chat");
  const form = document.getElementById("chat-form");
  const input = document.getElementById("message");
  const button = form.querySelector('button[type="submit"]');
  const micBtn = document.getElementById("mic-btn");
  const handsFreeBtn = document.getElementById("handsfree-btn");
  const stopTtsBtn = document.getElementById("stop-tts-btn");
  const pauseTtsBtn = document.getElementById("pause-tts-btn");
  const resumeTtsBtn = document.getElementById("resume-tts-btn");

  let currentAudio = null;
  let currentAudioUrl = null;

  function setTtsControls({ active, paused }) {
    stopTtsBtn.disabled = !active;
    pauseTtsBtn.disabled = !active || paused;
    resumeTtsBtn.disabled = !active || !paused;
  }

  function stopTts() {
    try {
      if ("speechSynthesis" in window) {
        window.speechSynthesis.cancel();
      }
      if (currentAudio) {
        currentAudio.pause();
        currentAudio.currentTime = 0;
        currentAudio = null;
      }
      if (currentAudioUrl) {
        URL.revokeObjectURL(currentAudioUrl);
        currentAudioUrl = null;
      }
    } finally {
      setTtsControls({ active: false, paused: false });
    }
  }

  function pauseTts() {
    let paused = false;
    try {
      if ("speechSynthesis" in window && window.speechSynthesis.speaking && !window.speechSynthesis.paused) {
        window.speechSynthesis.pause();
        paused = true;
      } else if (currentAudio && !currentAudio.paused) {
        currentAudio.pause();
        paused = true;
      }
    } finally {
      if (stopTtsBtn.disabled === false) {
        setTtsControls({ active: true, paused });
      }
    }
  }

  async function resumeTts() {
    let paused = false;
    try {
      if ("speechSynthesis" in window && window.speechSynthesis.paused) {
        window.speechSynthesis.resume();
        paused = false;
      } else if (currentAudio && currentAudio.paused) {
        await currentAudio.play();
        paused = false;
      }
    } catch (e) {
      // fall through
    } finally {
      if (stopTtsBtn.disabled === false) {
        setTtsControls({ active: true, paused });
      }
    }
  }

  function handleVoiceControlCommand(transcriptRaw) {
    const t = (transcriptRaw || "").trim().toLowerCase();
    if (!t) return false;

    const stopPhrases = new Set(["stop", "stop speaking", "stop tts"]);
    const pausePhrases = new Set(["pause", "pause speaking", "pause tts"]);
    const resumePhrases = new Set(["continue", "resume", "continue speaking", "resume speaking", "continue tts", "resume tts"]);

    const handsFreeOnPhrases = new Set([
      "hands free on",
      "handsfree on",
      "hands-free on",
      "enable hands free",
      "enable handsfree",
    ]);
    const handsFreeOffPhrases = new Set([
      "hands free off",
      "handsfree off",
      "hands-free off",
      "disable hands free",
      "disable handsfree",
    ]);

    if (stopPhrases.has(t)) {
      stopTts();
      return true;
    }
    if (pausePhrases.has(t)) {
      pauseTts();
      return true;
    }
    if (resumePhrases.has(t)) {
      // resume can be async; fire-and-forget for UI
      resumeTts();
      return true;
    }

    if (handsFreeOnPhrases.has(t)) {
      setHandsFree(true);
      return true;
    }

    if (handsFreeOffPhrases.has(t)) {
      setHandsFree(false);
      return true;
    }

    return false;
  }

  let recorder = null;
  let recording = false;
  let chunks = [];

  // Hands-free mode (wake word: "yow alfred")
  const WAKE_PHRASE = "yow alfred";
  let handsFreeEnabled = false;
  let handsFreeStream = null;
  let handsFreeLoopBusy = false;
  let handsFreeNextIsCommand = false;

  // Simple voice activity detection (VAD) to avoid calling /stt when the mic is silent.
  let handsFreeAudioCtx = null;
  let handsFreeAnalyser = null;
  let handsFreeSource = null;

  function getRmsLevel(analyser) {
    const data = new Uint8Array(analyser.fftSize);
    analyser.getByteTimeDomainData(data);
    let sumSq = 0;
    for (let i = 0; i < data.length; i++) {
      const v = (data[i] - 128) / 128;
      sumSq += v * v;
    }
    return Math.sqrt(sumSq / data.length);
  }

  async function isLikelySpeech() {
    if (!handsFreeAnalyser) return true;
    // Sample a few frames.
    let max = 0;
    for (let i = 0; i < 4; i++) {
      max = Math.max(max, getRmsLevel(handsFreeAnalyser));
      await new Promise((r) => setTimeout(r, 40));
    }
    // Threshold tuned to be conservative; adjust if too sensitive.
    return max > 0.02;
  }

  function updateHandsFreeUi() {
    handsFreeBtn.textContent = handsFreeEnabled ? "Handsâ€‘free: On" : "Handsâ€‘free: Off";
    handsFreeBtn.title = handsFreeEnabled
      ? `Handsâ€‘free listening is ON (wake phrase: \"${WAKE_PHRASE}\")`
      : "Toggle hands-free listening";

    micBtn.disabled = handsFreeEnabled;
    if (!recording) {
      micBtn.title = handsFreeEnabled ? "Handsâ€‘free is on (use wake phrase)" : "Click to record";
    }
  }

  function addBubble(text, role) {
    const div = document.createElement("div");
    div.className = `bubble ${role}`;
    div.textContent = text;
    chat.appendChild(div);
    chat.scrollTop = chat.scrollHeight;
  }

  async function playAlfredVoice(text) {
    // Always stop any existing playback first
    stopTts();
    setTtsControls({ active: true, paused: false });

    // Prefer browser TTS when available to avoid server API usage during development
    if ("speechSynthesis" in window) {
      try {
        const utter = new SpeechSynthesisUtterance(text);
        // Optional: tune voice, rate, pitch
        utter.lang = "en-US";
        utter.rate = 1.0;
        utter.onend = () => setTtsControls({ active: false, paused: false });
        utter.onerror = () => setTtsControls({ active: false, paused: false });
        window.speechSynthesis.speak(utter);
        return;
      } catch (err) {
        console.warn("Browser TTS failed, falling back to server TTS:", err);
      }
    }

    // Fallback: server-side TTS (requires OPENAI_API_KEY and server support)
    try {
      const res = await fetch(`${API_BASE}/tts`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text, voice: "alloy", format: "mp3" }),
      });

      if (!res.ok) {
        console.error("TTS error:", await res.text());
        return;
      }

      const blob = await res.blob();
      currentAudioUrl = URL.createObjectURL(blob);
      currentAudio = new Audio(currentAudioUrl);
      currentAudio.onended = () => {
        setTtsControls({ active: false, paused: false });
        try {
          if (currentAudioUrl) URL.revokeObjectURL(currentAudioUrl);
        } catch (e) {}
        currentAudioUrl = null;
        currentAudio = null;
      };
      await currentAudio.play();
    } catch (err) {
      console.error("Error playing TTS:", err);
      setTtsControls({ active: false, paused: false });
    }
  }

  stopTtsBtn.addEventListener("click", () => {
    stopTts();
  });

  pauseTtsBtn.addEventListener("click", () => {
    pauseTts();
  });

  resumeTtsBtn.addEventListener("click", () => {
    resumeTts();
  });

  form.addEventListener("submit", async (e) => {
    e.preventDefault();
    const text = input.value.trim();
    if (!text) return;

    addBubble(text, "user");
    input.value = "";
    input.focus();
    button.disabled = true;

    try {
      const res = await fetch(`${API_BASE}/chat`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ user_id: "default", message: text }),
      });
      const data = await res.json();
      addBubble(data.reply, "alfred");
      // Trigger Alfred's voice
      playAlfredVoice(data.reply);
    } catch (err) {
      addBubble("âš ï¸ Error talking to Alfred API.", "alfred");
      console.error(err);
    } finally {
      button.disabled = false;
    }
  });

  // ---- Voice input using API /stt via MediaRecorder ----
  function setMicUiActive(active) {
    recording = active;
    if (active) {
      micBtn.classList.add("listening");
      micBtn.textContent = "â—";
      micBtn.title = "Click to stop recording";
    } else {
      micBtn.classList.remove("listening");
      micBtn.textContent = "ðŸŽ¤";
      micBtn.title = "Click to record";
    }
  }

  async function sttFromBlob(blob, filename) {
    const fd = new FormData();
    fd.append("audio", blob, filename);

    const res = await fetch(`${API_BASE}/stt`, {
      method: "POST",
      body: fd,
    });

    if (!res.ok) {
      throw new Error(await res.text());
    }
    return res.json();
  }

  async function recordOnceFromStream(stream, durationMs) {
    return new Promise((resolve, reject) => {
      let localChunks = [];
      let localRecorder;
      try {
        localRecorder = new MediaRecorder(stream);
      } catch (e) {
        reject(e);
        return;
      }

      localRecorder.ondataavailable = (e) => {
        if (e.data && e.data.size > 0) localChunks.push(e.data);
      };
      localRecorder.onerror = (e) => reject(e);
      localRecorder.onstop = () => {
        const blob = new Blob(localChunks, { type: localRecorder.mimeType || "audio/webm" });
        resolve(blob);
      };

      localRecorder.start();
      setTimeout(() => {
        try {
          localRecorder.stop();
        } catch (e) {
          reject(e);
        }
      }, durationMs);
    });
  }

  function extractWakeMessage(transcriptLower) {
    // Accept small variations like "yo alfred" or "yoww alfred".
    const re = /\byo+w\s+alfred\b/i;
    const m = transcriptLower.match(re);
    if (!m) return null;
    const idx = transcriptLower.indexOf(m[0].toLowerCase());
    const after = transcriptLower.slice(idx + m[0].length).trim();
    return { wake: m[0], after };
  }

  async function handsFreeLoop() {
    if (!handsFreeEnabled || !handsFreeStream || handsFreeLoopBusy) return;
    handsFreeLoopBusy = true;
    try {
      // When not actively expecting a command, avoid calling /stt during silence.
      if (!handsFreeNextIsCommand) {
        const likelySpeech = await isLikelySpeech();
        if (!likelySpeech) return;
      }

      const blob = await recordOnceFromStream(handsFreeStream, 1200);
      const data = await sttFromBlob(blob, "handsfree.webm");
      const transcript = (data && data.text ? String(data.text) : "").trim();
      const transcriptLower = transcript.toLowerCase();

      if (!transcript) return;

      // Voice commands should work anytime.
      if (handleVoiceControlCommand(transcript)) return;

      // If we already heard the wake word, next transcript becomes the command.
      if (handsFreeNextIsCommand) {
        handsFreeNextIsCommand = false;
        input.value = transcript;
        form.dispatchEvent(new Event("submit"));
        return;
      }

      // Wake word routing.
      const wake = extractWakeMessage(transcriptLower);
      if (!wake) return;

      // If the user said: "yow alfred <message>", use the remainder.
      if (wake.after) {
        input.value = transcript.replace(new RegExp(wake.wake, "i"), "").trim();
        if (input.value) form.dispatchEvent(new Event("submit"));
        return;
      }

      // If only wake phrase was detected, treat the next chunk as the command.
      handsFreeNextIsCommand = true;
    } catch (err) {
      console.error("Hands-free STT loop error:", err);
      setHandsFree(false);
      addBubble("âš ï¸ Handsâ€‘free stopped (STT error).", "alfred");
    } finally {
      handsFreeLoopBusy = false;
      if (handsFreeEnabled) {
        setTimeout(handsFreeLoop, 400);
      }
    }
  }

  async function setHandsFree(enabled) {
    if (enabled === handsFreeEnabled) return;

    if (enabled) {
      if (!navigator.mediaDevices?.getUserMedia || !window.MediaRecorder) {
        addBubble("âš ï¸ Handsâ€‘free not supported in this browser.", "alfred");
        return;
      }

      try {
        // Stop any manual recording first.
        if (recorder && recording) {
          try { recorder.stop(); } catch (e) {}
        }

        handsFreeStream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // Set up analyser for a simple volume check.
        try {
          handsFreeAudioCtx = new (window.AudioContext || window.webkitAudioContext)();
          handsFreeSource = handsFreeAudioCtx.createMediaStreamSource(handsFreeStream);
          handsFreeAnalyser = handsFreeAudioCtx.createAnalyser();
          handsFreeAnalyser.fftSize = 1024;
          handsFreeSource.connect(handsFreeAnalyser);
          await handsFreeAudioCtx.resume();
        } catch (e) {
          // If WebAudio fails, we still can run hands-free without VAD.
          handsFreeAudioCtx = null;
          handsFreeSource = null;
          handsFreeAnalyser = null;
        }

        handsFreeEnabled = true;
        handsFreeNextIsCommand = false;
        updateHandsFreeUi();
        handsFreeLoop();
      } catch (err) {
        console.error("Hands-free mic permission error:", err);
        addBubble("âš ï¸ Microphone permission denied.", "alfred");
        handsFreeEnabled = false;
        handsFreeStream = null;
        updateHandsFreeUi();
      }
      return;
    }

    // disable
    handsFreeEnabled = false;
    handsFreeNextIsCommand = false;
    try {
      if (handsFreeAudioCtx) {
        handsFreeAudioCtx.close();
      }
    } catch (e) {}
    handsFreeAudioCtx = null;
    handsFreeSource = null;
    handsFreeAnalyser = null;
    try {
      if (handsFreeStream) {
        handsFreeStream.getTracks().forEach((t) => t.stop());
      }
    } catch (e) {}
    handsFreeStream = null;
    updateHandsFreeUi();
  }

  async function startRecording() {
    if (!navigator.mediaDevices?.getUserMedia || !window.MediaRecorder) {
      micBtn.disabled = true;
      micBtn.title = "Voice recording not supported in this browser";
      return;
    }

    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    chunks = [];
    recorder = new MediaRecorder(stream);

    recorder.ondataavailable = (e) => {
      if (e.data && e.data.size > 0) chunks.push(e.data);
    };

    recorder.onstop = async () => {
      try {
        stream.getTracks().forEach((t) => t.stop());
        const blob = new Blob(chunks, { type: recorder.mimeType || "audio/webm" });
        const data = await sttFromBlob(blob, "mic.webm");
        const transcript = (data && data.text ? String(data.text) : "").trim();
        if (!transcript) {
          addBubble("âš ï¸ No transcript returned.", "alfred");
          return;
        }

        // Voice-operated equivalents: allow voice control of TTS playback.
        if (handleVoiceControlCommand(transcript)) {
          return;
        }

        input.value = transcript;
        form.dispatchEvent(new Event("submit"));
      } catch (err) {
        console.error("STT error:", err);
        addBubble("âš ï¸ Error transcribing audio (STT).", "alfred");
      } finally {
        setMicUiActive(false);
        recorder = null;
      }
    };

    recorder.start();
    setMicUiActive(true);
  }

  function stopRecording() {
    if (recorder && recording) {
      recorder.stop();
    }
  }

  micBtn.addEventListener("click", async () => {
    try {
      if (!recording) {
        await startRecording();
      } else {
        stopRecording();
      }
    } catch (err) {
      console.error("Mic error:", err);
      setMicUiActive(false);
      addBubble("âš ï¸ Microphone permission or recording failed.", "alfred");
    }
  });

  handsFreeBtn.addEventListener("click", async () => {
    await setHandsFree(!handsFreeEnabled);
  });

  // Gesture: double-click / double-tap header toggles hands-free.
  (function initHandsFreeGesture() {
    const headerEl = document.querySelector("header");
    if (!headerEl) return;

    headerEl.addEventListener("dblclick", () => setHandsFree(!handsFreeEnabled));

    let lastTap = 0;
    headerEl.addEventListener(
      "touchend",
      () => {
        const now = Date.now();
        if (now - lastTap < 350) {
          setHandsFree(!handsFreeEnabled);
          lastTap = 0;
        } else {
          lastTap = now;
        }
      },
      { passive: true }
    );
  })();

  // Keyboard accessibility (when not typing):
  // - Esc: stop speaking
  // - P: pause
  // - R: resume
  // - H: toggle hands-free
  document.addEventListener("keydown", (e) => {
    const tag = (e.target && e.target.tagName) ? e.target.tagName.toLowerCase() : "";
    const typing = tag === "input" || tag === "textarea";
    if (typing) return;

    if (e.key === "Escape") {
      stopTts();
    } else if (e.key === "p" || e.key === "P") {
      pauseTts();
    } else if (e.key === "r" || e.key === "R") {
      resumeTts();
    } else if (e.key === "h" || e.key === "H") {
      setHandsFree(!handsFreeEnabled);
    }
  });

  // Init
  setMicUiActive(false);
  setTtsControls({ active: false, paused: false });
  updateHandsFreeUi();
  addBubble("Hello, I'm Alfred. You can type or tap ðŸŽ¤ to talk to me.", "alfred");
  </script>
</body>
</html>
